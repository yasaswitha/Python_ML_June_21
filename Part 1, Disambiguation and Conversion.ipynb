{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izK2ktqiIVzG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from functools import reduce\n",
        "import mafan\n",
        "from mafan import text\n",
        "import itertools\n",
        "import sys\n",
        "import os\n",
        "bos = \" <bos> \"\n",
        "eos = \" <eos> \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIg59eEFIVzL"
      },
      "source": [
        "# Tokenizer Functions\n",
        "\n",
        "## Sentence Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Llni8w7aIVzN"
      },
      "outputs": [],
      "source": [
        "def zng(paragraph):\n",
        "    for sent in re.findall(u'[^!?。\\.\\!\\?]+[!?。\\.\\!\\?]?', paragraph, flags=re.U):\n",
        "        yield sent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ckczbC_IVzO"
      },
      "source": [
        "## Simplified Chinese Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC6i5zNAIVzP"
      },
      "source": [
        "Below is the code for simplified to traditional mapping dictionary.\n",
        "\n",
        "We have a large dictionary *conversions.txt* that includes words, characters, common phrases, locations and idioms. Each entry contains the traditional chinese word and simplified chinese word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTFWWnGJIVzP"
      },
      "outputs": [],
      "source": [
        "infile = open(\"conversions.txt\", \"r+\", encoding=\"utf-8\")\n",
        "\n",
        "s2t_dict = dict()\n",
        "\n",
        "for line in infile:\n",
        "    line = line.rstrip()\n",
        "    arr = line.split()\n",
        "    trad = arr[0]\n",
        "    sim = arr[1]\n",
        "    if sim not in s2t_dict:\n",
        "        s2t_dict[sim] = [trad]\n",
        "    else:\n",
        "        s2t_dict[sim].append(trad)\n",
        "s2t_dict['-'] = ['-']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYBLUFglIVzQ"
      },
      "source": [
        "Tokeniser is used for identifying dictionary words and phrases in the input sentence. We always prefer longer phrases because it gives more meaning and less translation mappings. Hence we use Byte Pair Encoding (BPE) for identifying words, while BPE candidates are constrained by the defined list of vocabs in the dictionary. Since the longest phrase in the dictionary has 8 characters we start with 8-character phrases and do it backwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v40by492IVzS"
      },
      "outputs": [],
      "source": [
        "def tokenizer(sentence, n = 8):\n",
        "    '''\n",
        "    This function tokenizes input sentences according to the dicitionary.\n",
        "    Input: a sentence or paragraph\n",
        "    Output: a list of tokens from the input in order according to the original paragraph; a list of non-chinese characters from the original text.\n",
        "    '''\n",
        "    text, charList = prepare(sentence)\n",
        "    token_list = []\n",
        "    input_text = text\n",
        "    for k in range(n, 0, -1):\n",
        "        candidates = [input_text[i:i + k] for i in range(len(input_text) - k + 1)]\n",
        "        for candidate in candidates:\n",
        "            if candidate in s2t_dict:\n",
        "                token_list.append(candidate)\n",
        "                input_text = re.sub(candidate, '', input_text)\n",
        "    final = sequencer(token_list, text)\n",
        "    return final, charList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6ZZ-jw1IVzT"
      },
      "outputs": [],
      "source": [
        "def output_list(sentence_list, char_list):\n",
        "    count = 0\n",
        "    original = [] # sentence we want to output\n",
        "    \n",
        "    for word in sentence_list:\n",
        "        if \"-\" in word:\n",
        "            original.append(list(char_list[count]))\n",
        "            count += 1\n",
        "        else:\n",
        "            original.append(word)\n",
        "    return original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfWi6eFjIVzU"
      },
      "outputs": [],
      "source": [
        "def output(sentence, char_list):\n",
        "    count = 0\n",
        "    original = \"\" # sentence we want to output\n",
        "\n",
        "    for char in list(sentence):\n",
        "        if char == \"-\":\n",
        "            original += char_list[count] # append character if non-chinese\n",
        "            count += 1\n",
        "        else:\n",
        "            original += char # append chinese\n",
        "    return original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPHMcQFFIVzU"
      },
      "outputs": [],
      "source": [
        "def prepare(sentence):\n",
        "    new = \"\" # input to your tokenizer\n",
        "    char_list = [] # punct / english to be omitted\n",
        "\n",
        "    for char in list(sentence):\n",
        "        if text.identify(char) is mafan.NEITHER:\n",
        "            new += \"-\" # sub - with non-chinese chars\n",
        "            char_list.append(char)\n",
        "        else:\n",
        "            new += char\n",
        "\n",
        "    return new, char_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KemmTYaSIVzV"
      },
      "outputs": [],
      "source": [
        "def sequencer(tokens, example):\n",
        "\n",
        "    flags = [1] * len(example)\n",
        "    sequence = []\n",
        "    for token in tokens:\n",
        "        for match in re.finditer(token, example):\n",
        "            location = (token, match.span()[0], match.span()[1])\n",
        "            valid = reduce(lambda x,y:x*y, flags[location[1]:location[2]])\n",
        "            if valid:\n",
        "                sequence.append(location)\n",
        "                for i in range(location[1], location[2]):\n",
        "                    flags[i] = 0\n",
        "            else:\n",
        "                continue\n",
        "    sequence.sort(key=lambda x: x[1])\n",
        "    result = [x[0] for x in sequence]\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqVWNnfWIVzV"
      },
      "source": [
        "## Corpus Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkAOurxfIVzW"
      },
      "source": [
        "First, we need to prepare our corpus.\n",
        "1. We will add paddings (sentinels) to our sentences.\n",
        "2. Take one sentence at a time.\n",
        "3. Change non-chinese words to FW to avoid data explosion.\n",
        "4. Slice the n-grams and add them to dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrYHlx8eIVzW"
      },
      "outputs": [],
      "source": [
        "def add_stuff(order):\n",
        "    '''\n",
        "    This function divides the corpus into n-grams and stores them in dictionary.\n",
        "    Input: order of n-gram (like 2 for bi-gram)\n",
        "    Output: none\n",
        "    '''\n",
        "    infile = open(\"hk-zh.txt\", \"r+\") # this contains our corpus\n",
        "    start_padding = bos * order # add padding\n",
        "    end_padding = eos * order\n",
        "\n",
        "    for line in tqdm(infile, total=1314726):\n",
        "        line = line.rstrip()\n",
        "        sentences = list(zng(line)) # tokenize sentence by sentence\n",
        "        for sentence in sentences:\n",
        "            candidate = start_padding + sentence + end_padding # form sentence\n",
        "            word_list = candidate.split()\n",
        "            word_list_tokens = []\n",
        "            for word in word_list:\n",
        "                if not(bool(re.match('^[a-zA-Z0-9]+$', word))):\n",
        "                    word_list_tokens.append(word) # add if not chinese\n",
        "                else:\n",
        "                    word_list_tokens.append(\"FW\") # turn non-chinese (except punc) to FW\n",
        "            word_list = word_list_tokens\n",
        "            ordered = [word_list[i:i + order] for i in range(1, len(word_list) - order)] # extract n-grams through slicing\n",
        "            # for each ngram, convert to tuple and add to dictionary\n",
        "            for ngram in ordered:\n",
        "                ngram = tuple(ngram)\n",
        "                if ngram not in corpus:\n",
        "                    corpus[ngram] = 1\n",
        "                else:\n",
        "                    corpus[ngram] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "totEHVPvIVzX"
      },
      "source": [
        "Let's say you want to extract till trigrams.\n",
        "\n",
        "We want to do 3 iterations, for trigram, bi-gram and then unigram. Each iteration takes 2 minutes. This is only time-consuming part of this code. Once you prep the dictionary, you don't need to do this again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07yf4WvyIVzX"
      },
      "outputs": [],
      "source": [
        "corpus = dict()\n",
        "start_order = 3\n",
        "for i in range(start_order, 0, -1):\n",
        "    add_stuff(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AmmLAF1IVzX"
      },
      "source": [
        "Once you made the dictionary, dump it into a pickle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGAEXGlfIVzX"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('corpus.pkl', 'wb') as handle:\n",
        "    pickle.dump(corpus, handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp6pfVU_IVzY"
      },
      "source": [
        "Here's a way to load a pickle so you don't need to process data everytime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttEytQF-IVzY"
      },
      "outputs": [],
      "source": [
        "with open('corpus.pkl', 'rb') as fp:\n",
        "    corpus = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGbuQWOeIVzY"
      },
      "source": [
        "# Making Candidate Lists\n",
        "\n",
        "1. Tokenize the input.\n",
        "2. Check the mappings of each input.\n",
        "3. Add all possible mappings to candidate list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMeUYZEHIVzY"
      },
      "outputs": [],
      "source": [
        "def convert(sentence):\n",
        "    '''\n",
        "    Returns list of possible mappings.\n",
        "    Input: Simplified chinese sentence\n",
        "    Output: List of lists. Each list has a set of possible traditional chinese tokens\n",
        "    '''\n",
        "    tokens, char_list = tokenizer(sentence)\n",
        "    candidate_list = []\n",
        "    for token in tokens:\n",
        "        candidate_list.append(s2t_dict[token])\n",
        "    candidate_list = output_list(candidate_list, char_list)\n",
        "    return(candidate_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2XgZga1IVzZ"
      },
      "source": [
        "# Maximum log-likelihood calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZoDwEL0IVzZ"
      },
      "source": [
        "Compute the log likelihood of a sentence with different \\\\(\\alpha\\\\) penalties for unigram, bigram and trigrams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pMC_O-hIVzZ"
      },
      "outputs": [],
      "source": [
        "num_tokens = 4526000 # total number of tokens in corpus\n",
        "\n",
        "def prob(word_list, alpha_0 = 0.25, alpha_1 = 0.5, alpha_2 = 1.0):\n",
        "    '''\n",
        "    Computes the log likelihood probability.\n",
        "    Input: A sequence of words in form of list\n",
        "    Output: Log probabilties\n",
        "    '''\n",
        "    word_list = tuple(word_list) # change word list to tuple\n",
        "    if word_list in corpus:\n",
        "        # word found in dictionary\n",
        "        numerator = corpus[word_list] # get the frequency of that word list\n",
        "        denominator = num_tokens # let denominator be num tokens\n",
        "        # cutoff the last word and check whether it's in corpus\n",
        "        if len(word_list[:-1]) > 1 and word_list[:-1] in corpus:\n",
        "            denom_list = word_list[:-1]\n",
        "            denominator = corpus[denom_list]\n",
        "        if len(word_list[:-1]) == 1 and word_list[:-1] in corpus:       \n",
        "            return alpha_0 * log(numerator / denominator) # log of prob*alpha\n",
        "        elif len(word_list[:-1]) == 2 and word_list[:-1] in corpus:\n",
        "            return alpha_1 * log(numerator / denominator)\n",
        "        elif len(word_list[:-1]) == 3 and word_list[:-1] in corpus:\n",
        "            return alpha_2 * log(numerator / denominator)\n",
        "        else:\n",
        "            return log(numerator/denominator)\n",
        "    else:\n",
        "        word_list = list(word_list) # convert it back to list\n",
        "        k = len(word_list) - 1 # backoff, reduce n gram length\n",
        "        if k > 0:\n",
        "            # recursive function, divide the sequence into smaller n and find probs\n",
        "            probs = [prob(word_list[i:i + k]) for i in range(len(word_list) - k + 1)]\n",
        "            return sum(probs)\n",
        "        else:\n",
        "            # we found an unseen word\n",
        "            if not(bool(re.match('^[a-zA-Z0-9]+$', word_list[0]))):\n",
        "                return log(1 / num_tokens) # return a small probability\n",
        "            else:\n",
        "                return prob([\"FW\"]) # we encountered a non-chinese word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgZA252UIVza"
      },
      "source": [
        "# Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc9vVj24IVza"
      },
      "source": [
        "Generative n-gram language model that estimates the conditional probability of a word given its history in the n-gram. It's calculated by backing off through progressively shorter history models.\n",
        "\n",
        "Stupid Backoff does not generate normalized probabilities. The main difference is that we don’t apply any discounting and instead directly use the relative frequencies (S is used instead of P to emphasize that these are not probabilities but scores).\n",
        "\n",
        "\\\\(S(w^i|w^{i−1}_{i−k+1}) = \n",
        "\\begin{cases}\n",
        "    \\frac{f(w^{i}_{i−k+1})}{f(w^{i-1}_{i−k+1})} & \\text{if } f(w^{i}_{i−k+1})> 0\\\\\n",
        "    \\\\\\alpha S(w^i|w^{i−1}_{i−k+2}),              & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\\\)\n",
        "\n",
        "Where \\\\(\\alpha\\\\) is the backoff factor.\n",
        "\n",
        "Stupid Backoff is inexpensive to calculate in a distributed environment while approaching the quality\n",
        "of Kneser-Ney smoothing for large amounts of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYC29zjaIVza"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "def backoff(sentence, order, alpha_0 = 0.25, alpha_1 = 0.5, alpha_2 = 1.0):\n",
        "    '''\n",
        "    Calcuates log likelihood using backoff language model\n",
        "    Input: Sentence and order of the n-gram\n",
        "    Output: Log prob of that sentence\n",
        "    '''\n",
        "    score = 0\n",
        "    sentences = list(zng(sentence)) # sentence tokenizer\n",
        "    for sentence in sentences:\n",
        "        start_padding = bos * order # beginning padding\n",
        "        end_padding = eos * order # ending padding\n",
        "        candidate = start_padding + sentence + end_padding # add paddings\n",
        "        word_list = candidate.split()\n",
        "        word_list_tokens = []\n",
        "        for word in word_list:\n",
        "            # append only non-chinese words\n",
        "            if not(bool(re.match('^[a-zA-Z0-9]+$', word))):\n",
        "                word_list_tokens.append(word)\n",
        "            else:\n",
        "                word_list_tokens.append(\"FW\")\n",
        "        word_list = word_list_tokens\n",
        "        ordered = [word_list[i:i + order] for i in range(1, len(word_list) - order)] # shingle into n-grams\n",
        "        probs = [prob(x, alpha_0, alpha_1, alpha_2) for x in ordered] # calculate probabilities\n",
        "        score += sum(probs) # final answer\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rygiAjiPIVzb"
      },
      "source": [
        "# Translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osi1-LyLIVzb"
      },
      "source": [
        "Take simplified sentence as input, generate candidate list for sentence.\n",
        "For words with many to one mappings add the candidate to a temporary sentence, calculate perplexity and choose the option which gives the lowest perplexity.\n",
        "\n",
        "Call function to add back spaces at the end and output the final sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUZ4hNkvIVzb"
      },
      "outputs": [],
      "source": [
        "def translate(sentence, alpha_0 = 0.25, alpha_1 = 0.5, alpha_2 = 1.0):\n",
        "    '''\n",
        "    Translate a given sentence to traditional\n",
        "    Input: Simplified Sentence\n",
        "    Output: Traditional Sentence\n",
        "    '''\n",
        "    candidates = convert(sentence) # get the candidate lists\n",
        "    final_sent = \"\"\n",
        "    for words in candidates:\n",
        "        if len(words) > 1:\n",
        "            # many to one mappings\n",
        "            score = -50000.0 # start with extreme negative value\n",
        "            likely = \"\"\n",
        "            for candidate in words:\n",
        "                temp = final_sent\n",
        "                temp = temp + \" \"  + candidate # add a candidate to temp sentence\n",
        "                current_score = backoff(temp, 3, alpha_0, alpha_1, alpha_2) # check perplexity\n",
        "                if current_score > score:\n",
        "                    # if performing good, include that\n",
        "                    score = current_score\n",
        "                    likely = candidate\n",
        "            final_sent = final_sent + \" \" + likely\n",
        "        else:\n",
        "            final_sent = final_sent + \" \" + words[0]\n",
        "    final_sent = final_sent.replace(\" \", \"\")\n",
        "    final_sent = add_back_spaces(sentence, final_sent) #call function to add the spaces back and output translation\n",
        "    return final_sent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6ACPOgPIVzb"
      },
      "source": [
        "Add spaces back by enumerating through the original and the appended list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXPLiIvaIVzb"
      },
      "outputs": [],
      "source": [
        "def add_back_spaces(original, current):\n",
        "    current_list = list(current)\n",
        "    original_list = list(original)\n",
        "    count = 1\n",
        "    for index, char in enumerate(original_list):\n",
        "        if char == \" \":\n",
        "            current_list[index - count] += \" \"\n",
        "            count += 1\n",
        "    current = \"\".join(current_list)\n",
        "    return current"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7k0-6rIIVzc"
      },
      "source": [
        "Test sentence for translate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUZ8ztgWIVzc"
      },
      "outputs": [],
      "source": [
        "sentence = \"早在23岁，伍兹就参与了世界上首个核反应堆Chicago Pile-1的建设，她是导师费米领导的项目团队中最年轻的一员。此外，伍兹在建立和使用实验所需的盖革计数器上起到关键作用。反应堆成功运转并达到自持状态时，她也是唯一在场的女性。曼哈顿计划中，她与费米合作；同时，她曾与第一任丈夫约翰·马歇尔（John Marshall）一同解决了汉福德区钚生产厂氙中毒的问题，并负责监督钚生产反应炉的建造和运行。\"\n",
        "a = translate(sentence)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cLk6kHrIVzc"
      },
      "source": [
        "# Joint Probability Based Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7anTRyo-IVzc"
      },
      "source": [
        "Greedy tokenizer would generally work for most of the cases, however, it could lead to an undesirable segmentation, due to the preference towards longer chunks.\n",
        "We propose a joint consideration for sub-word segmentation by considering both source and target sentences.\n",
        "\n",
        "A translator needs a source sentence $\\mathbf{S}$ consisting of segmentations where $\\mathbf{S} = s_0 s_1 \\dots s_n$ and a target sentence $\\mathbf{T}$ consisting of segmentations where $\\mathbf{T} = t_0 t_1 \\dots t_m$.\n",
        "\n",
        "We want to find optimal arrangement of $\\mathbf{S}$ which is $\\mathbf{S}^*$ and optimal arrangement of $\\mathbf{T}$ which is $\\mathbf{T}^*$. Mathematically:\n",
        "\\begin{align}\n",
        "\\label{eq1}\n",
        "    \\mathbf{S}^*, \\mathbf{T}^* = \\underset{{s_i \\in \\mathbf{S}, t_j \\in \\mathbf{T}}}{\\operatorname{argmax}} P(\\mathbf{S}, \\mathbf{T})\n",
        "\\end{align}\n",
        "where $P(\\mathbf{S}, \\mathbf{T})$ is the joint probability of sequences. \n",
        "\n",
        "We assume that the prior probabilities, which are $P(\\mathbf{S})$ and $P(\\mathbf{T})$, are language model based probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvZPrRuqIVzd"
      },
      "outputs": [],
      "source": [
        "def dp_tokenizer(sentence):\n",
        "    s = sentence\n",
        "    global orig_len\n",
        "    orig_len = len(s)\n",
        "    return segment(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9VNCt52IVzd"
      },
      "outputs": [],
      "source": [
        "model = kenlm.Model(\"sim_train.klm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqVwJn2tIVzd"
      },
      "outputs": [],
      "source": [
        "def memo(f):\n",
        "    \"Memoize function f, whose args must all be hashable.\"\n",
        "    cache = {}\n",
        "    def fmemo(*args):\n",
        "        if args not in cache:\n",
        "            cache[args] = f(*args)\n",
        "        return cache[args]\n",
        "    fmemo.cache = cache\n",
        "    return fmemo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39L02IobIVzd"
      },
      "outputs": [],
      "source": [
        "def splits(text, start=0, L=20):\n",
        "    \"Return a list of all (first, rest) pairs; start <= len(first) <= L.\"\n",
        "    return [(text[:i], text[i:]) \n",
        "            for i in range(start, min(len(text), L)+1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVV3bpONIVzd"
      },
      "source": [
        "We chose Viterbi for segmentation the given sentence. The scoring function is obtained from the constructed language models.\n",
        "\n",
        "Optimal segmentation depends on the following:\n",
        "<ol>\n",
        "    <li>language model score of source sentence of a candidate segment.</li>\n",
        "    <li>language model score of target sentence of a candidate segment.</li>\n",
        "    <li>item mapping conversions from source segment to target segment</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jeo2Y1Y9IVzd"
      },
      "outputs": [],
      "source": [
        "@memo\n",
        "def segment(text):\n",
        "    \"Return a list of words that is the most probable segmentation of text.\"\n",
        "    if not text: \n",
        "        return []\n",
        "    else:\n",
        "        candidates = ([first] + segment(rest) \n",
        "                      for (first, rest) in splits(text, 1))\n",
        "        return max(candidates, key=Pwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTzbmuQyIVze"
      },
      "source": [
        "To avoid OOVs as output segmentations, we imposed a penalty on OOV outputs, which is given by: $\\alpha \\times \\frac{\\texttt{len(segment)}}{\\texttt{len(sentence)}}$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85mp5evxIVze"
      },
      "outputs": [],
      "source": [
        "penalty_constant = 15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWpjmt-sIVze"
      },
      "outputs": [],
      "source": [
        "def Pwords(words):\n",
        "    \"Probability of words, assuming each word is independent of others.\"\n",
        "    sentence = \" \".join(words)\n",
        "    score = 0\n",
        "    words_ = ['<s>'] + sentence.split() + ['</s>']\n",
        "    for i, (prob, length, oov) in enumerate(model.full_scores(sentence)):\n",
        "        if oov:\n",
        "            penalty = len(words_[i+1]) / orig_len\n",
        "            score += penalty_constant * prob * penalty\n",
        "        else:\n",
        "            score += prob\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozNeoQWNIVze"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "alphanumerics = 'a-zA-Z0-9'\n",
        "known_stops = u'。。…！？'\n",
        "known_punctuation = u'／（）、，。：「」…。『』！？《》“”；’ ‘【】·〔〕'\n",
        "eng_punct = string.punctuation\n",
        "avoid = re.compile(\"([%s%s%s%s]+)\" % (alphanumerics, known_stops, known_punctuation, eng_punct))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAlVvrM-IVze"
      },
      "source": [
        "# Tokenize Sentence\n",
        "Tokenize sentence and output the tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uKvXkhCIVzf"
      },
      "outputs": [],
      "source": [
        "def tokenize_sentence(sentence):\n",
        "    split_words = re.split(avoid, sentence)\n",
        "    split_words_values = [(i, bool(re.search(avoid, i))) for i in split_words]\n",
        "    answer = []\n",
        "    for (word, value) in split_words_values:\n",
        "        segmented_text = []\n",
        "        if value == False:\n",
        "            orig_len = len(word)\n",
        "            segmented_text = dp_tokenizer(word)\n",
        "        else:\n",
        "            segmented_text = list(word)\n",
        "        for segs in segmented_text:\n",
        "            answer.append(segs)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fBTZuMLIVzf"
      },
      "outputs": [],
      "source": [
        "sentence = \"姚松炎、周庭势被「DQ」? 泛民质疑，政府再取消参选人资格涉政治筛选，要求律政司司长郑若骅解释法律理据。 有报道指，据全国人大常委会就《基本法》第一百零四条进行的释法，代表泛民参选立法会港岛及九龙西补选的香港众志周庭和被「DQ」前议员姚松炎，势被取消参选资格。律政司表示，法律政策专员黄惠冲将于稍后时间与泛民议员会面，确实时间待定。 民主派议员前晚在律政中心外静坐要求与律政司司长郑若骅会面不果后，昨在立法会召开记者招待会，要求郑就撤销参选人资格的理据，及其给予选举主任的法律意见作出详细交代。公民党议员郭荣铿批评，郑不向公众交代的做法是「冇承担，冇责任」的表现，不能只把责任交托予公务员。\"\n",
        "a = tokenize_sentence(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Gq__tqIVzf"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCv-kkFIIVzf"
      },
      "source": [
        "Evaluating the accuracy on 100 lines on sample test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lyah3lUiIVzf"
      },
      "outputs": [],
      "source": [
        "sim_filename = \"simplified100.txt\"\n",
        "tra_filename = \"traditional100.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ujhC73TIVzf"
      },
      "outputs": [],
      "source": [
        "checklist = []\n",
        "for key in s2t_dict:\n",
        "    if len(s2t_dict[key]) > 1:\n",
        "        for t in s2t_dict[key]:\n",
        "            checklist.append(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXaOa-Z8IVzg"
      },
      "source": [
        "The translated characters are matched with the original traditional corpus during evaluation. The mismatch characters include wrongly characters and variant characters. Variant characters are characters that are homophones and synonyms. In some cases, simplified characters can have multiple traditional variant characters mapped to them, which gives the same meaning and context. Thus a mismatch in this case does not necessarily means a incorrect conversion. However this is not a common case and does not affect the evaluation result substantially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjPQI827IVzg"
      },
      "outputs": [],
      "source": [
        "def eval(sim_filename, tra_filename, alpha_0 = 0.25, alpha_1 = 0.5, alpha_2 = 1.0):\n",
        "    \n",
        "    total = 0\n",
        "    correct = 0\n",
        "    wrong = 0\n",
        "    micro_total = 0\n",
        "    micro_correct = 0\n",
        "    \n",
        "    sim_file = open(sim_filename, \"r+\", encoding=\"utf-8\")\n",
        "    tra_file = open(tra_filename, \"r+\", encoding=\"utf-8\")\n",
        "    tra_lines = tra_file.readlines()\n",
        "    line_count = 0\n",
        "\n",
        "    for line in sim_file:\n",
        "\n",
        "        line = line.rstrip()\n",
        "\n",
        "        line = translate(line, alpha_0 , alpha_1 , alpha_2)\n",
        "        tra_line = tra_lines[line_count].rstrip()\n",
        "\n",
        "        if len(line) == len(tra_line):\n",
        "            char_count = 0\n",
        "            for c in line:\n",
        "                total = total + 1\n",
        "                if c == tra_line[char_count]:\n",
        "                    correct = correct + 1\n",
        "                else:\n",
        "                    # print(c + tra_line[char_count])\n",
        "                    wrong = wrong + 1\n",
        "\n",
        "                if tra_line[char_count] in checklist:\n",
        "                    micro_total += 1\n",
        "                    if c == tra_line[char_count]:\n",
        "                        micro_correct = micro_correct + 1\n",
        "\n",
        "                char_count = char_count + 1\n",
        "\n",
        "        line_count += 1\n",
        "    results = []\n",
        "    results.append(('Total', (total)))\n",
        "    results.append(('Correct' , (correct)))\n",
        "    results.append(('Wrong' , (wrong)))\n",
        "    results.append(('Percentage' , (correct/total*100)))\n",
        "    results.append(('Micro Total' , (micro_total)))\n",
        "    results.append(('Micro Correct' , (micro_correct)))\n",
        "    results.append(('Micro Percentage' , (micro_correct/micro_total*100)))\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDE_fovYIVzg"
      },
      "source": [
        "Overall Accuracy is defined as (no. of correctly converted characters) / (no. of converted characters). We also calculate the Micro-average accuracy to evaluate the performance for one-to-many character conversions only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqvq_pKMIVzg"
      },
      "outputs": [],
      "source": [
        "print(eval(sim_filename, tra_filename, 0.7, 0.5, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX-kv9CnIVzh"
      },
      "source": [
        "# Hyper-Parameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZKh9AqrIVzh"
      },
      "source": [
        "Testing the evaluation function on the test set with multiple hyperparameter values in order to determine the optimal values for the hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5IyQYvRIVzi"
      },
      "outputs": [],
      "source": [
        "outfile = open('alpha_tuning.txt', 'w+')\n",
        "max = [99.5, 0.25, 0.5, 1]\n",
        "for a_0 in range(100, 10, -10):\n",
        "    for a_1 in range(100, 10, -10):\n",
        "        outfile.write(str(eval(sim_filename, tra_filename, alpha_0 = float(a_0)/100.0, alpha_1 = float(a_1)/100.0, alpha_2 = 1.0)) + \"a0: \" + str(a_0/100.0) + \" \" + \"a1: \" + str(a_1/100.0) + '\\n')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Part 1, Disambiguation and Conversion.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}